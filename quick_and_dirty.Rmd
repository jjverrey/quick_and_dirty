---
title: "Quick and Dirty"
author: "Jacob Verrey"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

#Part I: Maze Performance Analysis
```{r}
masterData <- read.csv("v2_mover_date_master.csv")

#**********************************
#Create Derived Metrics
#**********************************

#Improvement Index
masterData$first_half_sec_half_improvement <- 
  masterData$second_halv_avg_speed - masterData$first_half_avg_speed


#**********************************
#Initialize Z Score Values
#**********************************

#Completion Time Z scores
masterData$completion_time_z <- 
  (masterData$completion_time - mean(masterData$completion_time)) / sd(masterData$completion_time)

#Total Avg Speed Z scores
masterData$total_avg_speed_z <- 
  (masterData$total_avg_speed - mean(masterData$total_avg_speed)) /sd(masterData$total_avg_speed)

#Improvement Z scores
masterData$first_half_sec_half_improvement_z <-
(masterData$first_half_sec_half_improvement -
mean(masterData$first_half_sec_half_improvement))/sd(masterData$first_half_sec_half_improvement)

```

## 1) How did people perform on the maze? Are there any glitches?

```{r}
#Let's see if completoin time predicts bonus..
plot(x=masterData$completion_time, y = masterData$cash_bonus, ylab = "Cash Bonus ($)",
     main = "Completion Time vs. Cash Bonus", xlab = "Completion Time (s)")
```
```{r}
cor(masterData$completion_time, masterData$cash_bonus)
```

Very good! The longer you take, the worse your cash bonus is, which is what my formula predicts (bonus = 2 - elapsed_time_in_seconds * .01). This suggests that there was no major glitches when I launched this task on mTurk.

## 2) How did people perform on this maze?

```{r}
par(mfrow = c(1,2))
boxplot(masterData$completion_time, main = "Distribution of Completion Time",
        ylab = "Completion Time (s)")
hist(masterData$completion_time, main = "Frequency vs. Completion Time Bins",
     xlim=c(0,200), xlab = "Completion Time (s)")
abline(v=mean(masterData$completion_time))
par(mfrow = c(1,1))
```
```{r}
summary(masterData$completion_time)
```

Ok - people took a little longer than a minute and a half, on average, to complete the maze, with a pretty good distribution of scores.

## 3) Was maze performance affected by where the participant started? (center vs. edge)

```{r}
#Graph it
boxplot(masterData$completion_time ~ masterData$start_from_center, xlab = "Starting pos",
        main = "Distribution of Completion Time by starting pos",
        names = c("edge", "center"), ylab = "Completion Time (s)")

?boxplot
```
```{r}
#Take difference of means
mean(subset(masterData, start_from_center)$completion_time) - mean(subset(masterData, !start_from_center)$completion_time)
```

Ok - so it looks like starting from the edge makes you perform ~16 seconds better than starting from the center. Let's see if it's signifcant...

```{r}
#Take the difference of means via a ttest

t.test(masterData$completion_time ~ masterData$start_from_center)$p.value
```

Not significant, but I'll be curious to see what happens to it when I get more data.

## 4) Is speed (a metric I made up) & speed-improvement a good proxy for performance?

Speed is in a metric called 'checkpoints/second', which is not very intuitive. Because of this, I will use z-scores instead, as everyone knows how to interpret a z-score.

```{r}
#Plot out completoin time vs. speed (z)
plot(completion_time ~ total_avg_speed_z, data = masterData,
main = "Completion Time vs. Avg Speed Zs", ylab = "Completoin Time (S)",
xlab = "Avg Speed Zs")
regression <- lm(completion_time ~ total_avg_speed_z, data = masterData)
abline(regression)
abline(h = mean(masterData$completion_time), col = "grey")
```

```{r}
coef(regression)
```

```{r}
summary(regression)$r.squared
```

Wow! So a one standard deviation increase in overall average speed will make you perform 30 seconds better on the maze! Additionally, my speed metric predicts a ton of the variance (~65%)! Speed seems to be an excellent predicter for performance!

Now how does learning (i.e. speed improvement) affect performance?

```{r}
#Learning vs. completion time
plot(completion_time ~ first_half_sec_half_improvement_z, data = masterData,
main = "Completion Time vs. Learning Zs", ylab = "Completoin Time (S)",
xlab = "Speed improvement from first half to second half Zs")
regression <- lm(completion_time ~ first_half_sec_half_improvement_z,
data = masterData)
abline(h = mean(masterData$completion_time), col = "gray")
abline(regression)
```
```{r}
coef(regression)
```

```{r}
summary(regression)$r.squared
```

Hmm - so this relationship isn't exactly linear: there's a group of people near 1 sd below the mean who did outrageously bad. Excludign them thoguh, the relationship seems linear: the more one improves, the better they'll do on the maze by roughly 17 seconds. This isn't as pwoerful as a predicto rof overall speed, but it's nice to see that improvement explains some of the data.

## 5) How do other metrics affect maze performance? Like pixel movement & practice.

```{r}
#Does total pixel movement (i.e. efficiency) predict maze performance?
plot(completion_time ~ maze_total_pixel_movement, data = masterData,
main = "Completion Time vs. Total pixel movement", ylab = "Completoin Time (s)",
xlab = "Maze total pixel movement")
regression <- lm(completion_time ~ maze_total_pixel_movement, data = masterData)
abline(regression)
```

```{r}
coef(regression)
```

```{r}
summary(regression)$r.squared
```

Ok, so for each pixel the user moves, it costs the user an extra .06 seconds on the maze. This makes sense, since the less pixels you use to move, the more efficent you'll complete the maze.

```{r}
#Does total practice (seconds) predict maze performance?
plot(completion_time ~ practice_time, data = masterData,
main = "Completion Time vs. Practice (time)", ylab = "Completoin Time (s)",
xlab = "Amount of practice (s)")
regression <- lm(completion_time ~ practice_time, data = masterData)
```

```{r}
coef(regression)
```

```{r}
summary(regression)$r.squared
```

Wow! So for every second a user practices, he's expected to take .01 seconds longer to complete the maze. This seems a bit fishy: this study was launched on mTurk, and mTurkers often pause halfway through a survey to complete other surveys. The people who spend logner on the practice maze could just be multitasking mTurkers who go to another survey before going back to our survey, and of course they would do worse on a maze: if you're doing another survey and taking a break off the maze game, then when you come back, you'll likely forget how to move, and you therefore won't do that well on the real maze.

Why don't we look at osmething like total pixel movement?

```{r}
#Look at total pixel movement
plot(completion_time ~ practice_total_pixel_movement, data = masterData,
main = "Completion Time vs. Practice", ylab = "Completoin Time (s)",
xlab = "Amount of practice (total pixel movement)")
regression <- lm(completion_time ~ practice_total_pixel_movement, data = masterData)
abline(regression)
```

```{r}
coef(regression)
```

```{r}
summary(regression)$r.squared
```

Ok, so for every pixel the user moves, he will perform .006 seconds better at the maze, so if he moves 500 pixels (most of the users move less than 500 pictures), you'll only get aorund 3 seconds better at the maze. This is a really small effect however, so I don't think that's strong evidence that practice, in terms of pixel movement, predicts performance. Perhaps this is jus tbecause of self selection: people who are really bad at the maze tend to practice more so they're less bad, or about as good as the average participatn who doesn't practice, so self-selection may correct for the skill-boosting effect of practice

## 6) let's see how all these independent maze predicters relate....

```{r}
multi_var_regr <- lm(completion_time ~ total_avg_speed_z +
first_half_sec_half_improvement_z +
total_reset_count + maze_total_pixel_movement, data = masterData)
summary(multi_var_regr)
```

Very good! Controllign forother variables, it seems that 

# Part 2: Mover Psyche.

## Part 0: Recode some variables

```{r}

# "Compared to most people, how skilled do you think you are?"
masterData$self_skill_avg_q <- NA
masterData$self_skill_avg_q[masterData$self_skill_avg == "I am less skilled than most people"] <- -1
masterData$self_skill_avg_q[masterData$self_skill_avg == "I am about as skilled as most people"] <- 0
masterData$self_skill_avg_q[masterData$self_skill_avg == "I am more skilled than most people"] <- 1

# "Compared to MOST PEOPLE, how skilled do you think your partner is?"
masterData$partner_skill_avg_q <- NA
masterData$partner_skill_avg_q[masterData$partner_skill_avg == "They are less skilled than most people"] <- -1
masterData$partner_skill_avg_q[masterData$partner_skill_avg == "They are about as skilled as most people"] <- 0
masterData$partner_skill_avg_q[masterData$partner_skill_avg == "They are more skilled than most people"] <- 1

# "Compared to YOU, how skilled do you think your partner is?
masterData$partner_skill_you_q <- NA
masterData$partner_skill_you_q[masterData$partner_skill_you == "They are less skilled than me"] <- -1
masterData$partner_skill_you_q[masterData$partner_skill_you == "They are about as skilled as me"] <- 0
masterData$partner_skill_you_q[masterData$partner_skill_you == "They are more skilled than me"] <- 1


# "How often were you thinking about switching?"
masterData$switch_thinking_q <- NA
masterData$switch_thinking_q[masterData$switch_thinking == "Never"] <- 0
masterData$switch_thinking_q[masterData$switch_thinking == "Some of the time"] <- 1
masterData$switch_thinking_q[masterData$switch_thinking == "About half the time"] <- 2
masterData$switch_thinking_q[masterData$switch_thinking == "Most of the time"] <- 3
masterData$switch_thinking_q[masterData$switch_thinking == "The entire time"] <- 4

#Assign them to a catagory based on skill
masterData$skill_catagory <- NA
masterData$skill_catagory[masterData$completion_time_z <= -2] <- "ERROR" #IMPOSSIBLE to get a score this good!
masterData$skill_catagory[masterData$completion_time_z <= -1 & masterData$completion_time_z > -2] <- "-2_SD"
masterData$skill_catagory[masterData$completion_time_z <= 0 & masterData$completion_time_z > -1] <- "-1_SD"
masterData$skill_catagory[masterData$completion_time_z > 0 & masterData$completion_time_z < 1] <- "1_SD"
masterData$skill_catagory[masterData$completion_time_z >= 1 & masterData$completion_time_z < 2] <- "2_SD"
masterData$skill_catagory[masterData$completion_time_z >= 2] <- "3_SD"

# and relevel them so -2_sd is the first thing tha'ts displayed (vs. -1_sd)
masterData$skill_catagory <- as.factor(masterData$skill_catagory)
masterData$skill_catagory <- relevel(masterData$skill_catagory, "-2_SD")

```
## Part 1: Do the movers think they shoud've switched roles?

```{r}
par(mfrow = c(1,2))

#Should you ahve switched? Yes or no.
barplot(prop.table(table(masterData$switch_goodbad)), ylab = "% of respondents",
xlab = "Should you have switched?", names.arg = c("No", "Yes"),
main = "Should you have switched?")

switched_on_perf_self_report <- prop.table(table(masterData$noswitch_impact))
#Should you have switched? Effects of switchign on performance
barplot(switched_on_perf_self_report, ylab = "% of respondents",
xlab = "Affect of not switching on performance", main = "How'd not switch affect performance?")

# Assign them to a catagory based on their skill level
masterData$skill_catagory <- NA

par(mfrow = c(1,1))
```

Let's extract some numberas from the barchart.

```{r}
# proportion of people who claimed that not switching hurt, helped, or had no affect on performance
ns_hurt <- sum(switched_on_perf_self_report[0:5])
ns_zero <- switched_on_perf_self_report[6]
ns_helped <- sum(switched_on_perf_self_report[7:11])

paste(ns_hurt, ns_zero, ns_helped, sep = ", ") #Values pasted togehter so they take up one line
```

Ok - the values from the 'affect performance' histogram cnicely complement the 'should you have switched' histogram. More specifically, a little over 20% of people think that they should have switched with their partner, and in the 'affect performance' histogram, 20% of people think that not switching hurt performance. This all makes sense: if you think that not switching hurt your performance, then of c oursae you would've wanted to switch roles with your partner!

Roughly 70% of participants think that the switch should NOT have happened, but only 50% of participants think that switching would've hurt performance. This is weird: one would think that people who say that switching should NOT have happened should also say that they switching would have hurt perofrmance!

The missing 20% could come from people who responded '0' to the affect_peroformance question; those are the people who didn't think switching helped or hurt their performance. Most of the people who voted '0' in 'affect_performacne' histogram could've voted for 'no' in the 'should you have switched?' question. If they did, then this would explain the missing 20%, and more critically, this would mean that, if you think switching roles will have a NUETRAL affect on performance, then you won't switch.

```{r}
plot(as.factor(masterData$switch_goodbad), x= jitter(masterData$noswitch_impact), 
     xlab = "Affect of switching in performance", ylab="Should you have switched (y/n)",
     main = "Affect of switch performance predict binary view of switch?")
```

Yep, and it seems like one's view on ho wmuch switching helped/hurt performance very much affects whether you think you should have switched. It seems that those who think switching has no affect on performance ALSO think that you shouldn't switch... perhaps peopel only do switch if they think it'll have a positive - rather than nuetral - effect.

```{r}
switch_on_perf_self_report <- prop.table(table(masterData$noswitch_impact))

```
This graph paints a similar sotry: most people visually seem to think that not switching helped performance, with people seeming to think that it helped perofrmance by a good amount (i.e. many people responded with a value > 2)

## 2) Are people a goo djudge of their own performance?
```{r}
# How good are you at judging your own performance?
par(mfrow = c(1,2))
plot(y = masterData$completion_time, x = masterData$self_skill_avg, 
     main = "Maze performance perc: mover",
     names = c("same", "less skilled", "more skilled"))
barplot(table(masterData$self_skill_avg), names = c("same", "less skilled", "more skilled"))
par(mfrow = c(1,1))
```

Huh! It seems that people are pretty bad at predicting their own performance... people who rate themselves as 'as skilled as most people' and 'more skilled than most people' tend to rate the same way. People who are incompetent, however, have a whole range of beliefs, but to a lot of them, at least they know they're incompetant

```{r}
# How do you judge partner's performance?
par(mfrow = c(1,2))
plot(y = masterData$completion_time, x = masterData$partner_skill_you, 
     main = "Maze performance perc: mover",
     names = c("same", "less skilled", "more skilled"))
barplot(table(masterData$partner_skill_you), names = c("same", "less skilled", "more skilled"))
par(mfrow = c(1,1))
```

```{r}

# -- 2) Are people good judges of their performance?
plot(x = masterData$skill_catagory, y = masterData$)
# -- 2) Are people good judges of their performance?
plot(x = masterData$skill_catagory, y = masterData$self_skill_avg)
# -- 2) Are people good judges of their performance?
plot(x = masterData$skill_catagory, y = masterData$self_skill_avg_q)
# -- 2) Are people good judges of their performance?
boxplot(masterData$self_skill_avg_q ~ masterData$skill_catagory)
ox
# -- 2) Are people good judges of their performance?
boxplot(masterData$self_skill_avg_q ~ masterData$skill_catagory)
table(masterData$self_skill_avg)
# -- 2) Are people good judges of their performance?
boxplot(masterData$self_skill_avg_q ~ masterData$skill_catagory, xlab = "Skill catagory",
ylab="Skill lvl compared to partner")
table(masterData$self_skill_avg)
boxplot(masterData$self_skill_avg_q ~ masterData$skill_catagory, xlab = "Skill catagory",
ylab="Skill lvl compared to partner", main = "Am I good at the maze?")
#better_on_own: How much better would you be on your own (METRIC)? Absolute vs. present
masterData$better_on_own_seconds <- masterData$completion_time - masterData$perf_self_solo
masterData$better_on_own_percent <- (1 - masterData$perf_self_solo/masterData$completion_time) * 100
View(masterData)
masterData$better_on_own_percent <- round((1 - masterData$perf_self_solo/masterData$completion_time) * 100, 2)
#better_on_own: How much better would you be on your own (METRIC)? Absolute vs. present
masterData$better_on_own_seconds <- masterData$completion_time - masterData$perf_self_solo
masterData$better_on_own_percent <- round((1 - masterData$perf_self_solo/masterData$completion_time) * 100, 2)
masterData$myself_average_comparison_seconds <- masterData$completion_time - masterData$perf_avg_pair
masterData$myself_average_comparison_percent <- round((1 - masterData$perf_avg_pair/masterData$completion_time) * 100, 2)
View(masterData)
# - a) Are people good judges of their performance?
factors(masterData$self_skill_avg)
# - a) Are people good judges of their performance?
Table(masterData$self_skill_avg)
# - a) Are people good judges of their performance?
table(masterData$self_skill_avg)
boxplot(masterData$self_skill_avg_q ~ masterData$skill_catagory,
xlab = "Skill catagory (lower = better)", ylab="Skill lvl compared to most people",
main = "Am I good at the maze?")
# - a) Are people good judges of their performance? Metric approach.
boxplot(masterData$self_skill_avg_q ~ masterData$skill_catagory,
xlab = "Skill catagory (lower = better)", ylab="Skill lvl compared to most people",
main = "Am I good at the maze compared to most people?")

```

